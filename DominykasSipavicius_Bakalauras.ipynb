{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQzwg4mmCRPl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyuR04-SHcQ9"
      },
      "outputs": [],
      "source": [
        "!pip install mne\n",
        "import mne\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"This filename (.*) does not conform to MNE naming conventions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzvP-zD0HkIi"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(name):\n",
        "    parts = name.split('_')\n",
        "    class_name = parts[1]\n",
        "    return class_name\n",
        "\n",
        "def get_folder(name):\n",
        "    parts = name.split('_')\n",
        "    folder = parts[0]\n",
        "    return folder\n",
        "\n",
        "def read_eeg_data(file_path, folder):\n",
        "    raw = mne.io.read_raw_fif(file_path, preload=True, verbose = 40)\n",
        "\n",
        "    include_channels = [\"O1\", \"O2\", \"P3\", \"P4\", \"C3\", \"C4\", \"F3\", \"F4\"]\n",
        "    # include_channels = [\"F3\", \"F4\", \"C3\", \"C4\"]\n",
        "    raw.pick(picks=[ch for ch in raw.ch_names if ch in include_channels])\n",
        "    raw.filter(1, 100, verbose = 40)\n",
        "    raw.savgol_filter(15, verbose=40)\n",
        "    data_segments = raw.get_data()\n",
        "    # plot_trimmed_data(data_segments, raw.info['sfreq'], include_channels)\n",
        "\n",
        "    if data_segments.shape[1] >= 1225:\n",
        "        data_segments = data_segments[:, :1225]\n",
        "\n",
        "    elif data_segments.shape[1] < 1225:\n",
        "        raise ValueError(\"The data does not have enough time points. Required: 1225, Found: {}\".format(data_segments.shape[1]))\n",
        "\n",
        "    return data_segments\n",
        "\n",
        "def plot_trimmed_data(data_segments, sfreq, channel_names):\n",
        "\n",
        "    times = np.linspace(0, data_segments.shape[1] / sfreq, num=data_segments.shape[1])\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(data_segments.shape[0]):\n",
        "        plt.plot(times, data_segments[i] + i * 30)\n",
        "    plt.xlabel('Laikas (sek.)')\n",
        "    plt.yticks(np.arange(0, data_segments.shape[0] * 30, 30), channel_names)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gHG8e5YcId6P"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "labels = []\n",
        "folders = []\n",
        "folder_path = '/content/drive/MyDrive/newDataWithSeperation'\n",
        "\n",
        "\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith('.fif'):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        try:\n",
        "            label = remove_numbers(os.path.splitext(file)[0])\n",
        "            folder = get_folder(os.path.splitext(file)[0])\n",
        "            eeg_data = read_eeg_data(file_path, folder)\n",
        "\n",
        "            folder = get_folder(os.path.splitext(file)[0])\n",
        "\n",
        "            data.append(eeg_data)\n",
        "            labels.append(label)\n",
        "            folders.append(folder)\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping file {file}: {e}\")\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "folders = np.array(folders)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7mmCHtBSCN0"
      },
      "outputs": [],
      "source": [
        "def augment_data_by_reversing(data_segments, labels, folders):\n",
        "    channels = [\"O1\", \"O2\", \"P3\", \"P4\", \"C3\", \"C4\", \"F3\", \"F4\"]\n",
        "    for segment, label, folder in zip(data_segments, labels, folders):\n",
        "        augmented_data_segments.append(segment)\n",
        "        augmented_labels.append(label)\n",
        "        augmented_folders.append(folder)\n",
        "\n",
        "        reversed_segment = segment[:, ::-1]\n",
        "\n",
        "        augmented_data_segments.append(reversed_segment)\n",
        "        augmented_labels.append(label)\n",
        "        augmented_folders.append(folder)\n",
        "\n",
        "    return augmented_data_segments, augmented_labels\n",
        "\n",
        "augmented_data_segments = []\n",
        "augmented_labels = []\n",
        "augmented_folders = []\n",
        "augment_data_by_reversing(data, labels, folders)\n",
        "\n",
        "data = np.array(augmented_data_segments)\n",
        "labels = np.array(augmented_labels)\n",
        "folders = np.array(augmented_folders)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "print(folders.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iYBtqPnOISB"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Input, Conv1D, ReLU, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-jXojwoPXor"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'labels': labels,\n",
        "    'folder': folders\n",
        "})\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['labels_encoded'] = encoder.fit_transform(df['labels'])\n",
        "\n",
        "test_folders = ['1', '2']\n",
        "mask = df['folder'].isin(test_folders)\n",
        "\n",
        "X_train = data[~mask]\n",
        "y_train = df.loc[~mask, 'labels_encoded']\n",
        "X_test = data[mask]\n",
        "y_test = df.loc[mask, 'labels_encoded']\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsOUVkf2Pio5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv1D, ReLU, Flatten, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=4)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=4)\n",
        "\n",
        "num_channels = X_train.shape[1]\n",
        "num_time_points = X_train.shape[2]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=20, kernel_size=40, strides=2, padding='same', activation='relu', input_shape=(num_time_points, num_channels)))\n",
        "model.add(Conv1D(filters=10, kernel_size=20, strides=1, padding='same', activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "X_train = X_train.reshape([-1, 1225, num_channels])\n",
        "X_test = X_test.reshape([-1, 1225, num_channels])\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint_epoch_{epoch:02d}'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "history = model.fit(X_train, y_train_encoded, epochs=25, batch_size=50, validation_data=(X_test, y_test_encoded), callbacks=[model_checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhGpNk9GnID8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=4)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=4)\n",
        "\n",
        "num_channels = X_train.shape[1]\n",
        "num_time_points = X_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(filters=130, kernel_size=32, strides=1, padding='valid', activation='relu', input_shape=(num_time_points, num_channels)),\n",
        "    Flatten(),\n",
        "    Dense(152, activation='relu'),\n",
        "    Dense(150, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "X_train = X_train.reshape([-1, 1225, num_channels])\n",
        "X_test = X_test.reshape([-1, 1225, num_channels])\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint_epoch_{epoch:02d}'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "history = model.fit(X_train, y_train_encoded, epochs=25, batch_size=50, validation_data=(X_test, y_test_encoded), callbacks=[model_checkpoint_callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHeYDZtDuGbD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, MaxPooling2D, Conv2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=4)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=4)\n",
        "\n",
        "num_channels = X_train.shape[1]\n",
        "num_time_points = X_train.shape[2]\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(filters=60, kernel_size=(4, 4), activation='relu', strides=(1, 1), padding='same', input_shape=(num_time_points, num_channels, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(filters=50, kernel_size=(4, 4), activation='relu', strides=(1, 1), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "\n",
        "    Conv2D(filters=40, kernel_size=(10, 10), activation='relu', strides=(1, 1), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "\n",
        "    Conv2D(filters=40, kernel_size=(10, 10), activation='relu', strides=(1, 1), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "\n",
        "    Conv2D(filters=4, kernel_size=(15, 15), activation='relu', strides=(1, 1), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 1)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(50, activation='tanh'),\n",
        "    Dense(20, activation='tanh'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "X_train = X_train.reshape([-1, num_time_points, num_channels, 1])\n",
        "X_test = X_test.reshape([-1, num_time_points, num_channels, 1])\n",
        "\n",
        "checkpoint_filepath = '/tmp/checkpoint_epoch_{epoch:02d}'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "history = model.fit(X_train, y_train_encoded, epochs=25, batch_size=50, validation_data=(X_test, y_test_encoded), callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYEmclQyQJKv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Modelio tikslumas')\n",
        "plt.ylabel('Tikslumas')\n",
        "plt.xlabel('Epizodai')\n",
        "plt.legend(['Mok', 'Test'], loc='lower right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Modelio nuostoliai')\n",
        "plt.ylabel('Nuostoliai')\n",
        "plt.xlabel('Epizodai')\n",
        "plt.legend(['Mok', 'Test'], loc='upper right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyVQGbJlQLLb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "epoch_to_load = 11\n",
        "model.load_weights(f'/tmp/checkpoint_epoch_{epoch_to_load:02d}')\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for data_point, predicted_class in zip(X_test, predicted_classes):\n",
        "    data_point.reshape([-1, 1, 1225])\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_classes)\n",
        "\n",
        "labels_true = encoder.inverse_transform(range(len(encoder.classes_)))\n",
        "\n",
        "labels_true = [label.replace(\"Baseline\", \"Jokio\").replace(\"Desine\", \"Dešinė\") for label in labels_true]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels_true, yticklabels=labels_true, cmap=\"crest\")\n",
        "plt.xlabel('Spėjimas')\n",
        "plt.ylabel('Tiesa')\n",
        "plt.title('Klasifikavimo lentelė')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}